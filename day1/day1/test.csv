frame,new_link,paper_abstract,paper_title,paper_url_abs,paper_url_pdf,papers_with_code,repo_url,star_number,subtask,subtask_url,task
,,"    Automated construction of knowledge hierarchies from huge data corpora is gaining increasing attention in recent years, in order to tackle the infeasibility of manually extracting and semantically linking millions of concepts. As a knowledge hierarchy evolves with these automated techniques, there is a need for measures to assess its temporal evolution, quantifying the similarities between different versions and identifying the relative growth of different subgraphs in the knowledge hierarchy.In this paper, we focus on measures that leverage structural properties of the knowledge hierarchy graph to assess the temporal changes. We propose a principled and scalable similarity measure, based on Katz similarity between concept nodes, for comparing different versions of a knowledge hierarchy, modeled as a generic directed acyclic graph. We present theoretical analysis to depict that the proposed measure accurately captures the salient properties of taxonomic hierarchies, assesses changes in the ordering of nodes, along with the logical subsumption of relationships among concepts. We also present a linear time variant of the measure, and show that our measures, unlike previous approaches, are tunable to cater to diverse application needs. We further show that our measure provides interpretability, thereby identifying the key structural and logical difference in the hierarchies. Experiments on a real DBpedia and biological knowledge hierarchy showcase that our measures accurately capture structural similarity, while providing enhanced scalability and tunability. Also, we demonstrate that the temporal evolution of different subgraphs in this knowledge hierarchy, as captured purely by our structural measure, corresponds well with the known disruptions in the related subject areas.",Automated assessment of knowledge hierarchy evolution: comparing directed acyclic graphs,https://link.springer.com/article/10.1007/s10791-018-9345-y,https://www.researchgate.net/publication/329716534_Automated_assessment_of_knowledge_hierarchy_evolution_comparing_directed_acyclic_graphs,422,https://github.com/guruprasadnk7/DAGSimilarityKatz,1,Machine Translation,,"knowledge graph completion,knowledge graphs,question answering"
,,"    Automated construction of knowledge hierarchies from huge data corpora is gaining increasing attention in recent years, in order to tackle the infeasibility of manually extracting and semantically linking millions of concepts. As a knowledge hierarchy evolves with these automated techniques, there is a need for measures to assess its temporal evolution, quantifying the similarities between different versions and identifying the relative growth of different subgraphs in the knowledge hierarchy.In this paper, we focus on measures that leverage structural properties of the knowledge hierarchy graph to assess the temporal changes. We propose a principled and scalable similarity measure, based on Katz similarity between concept nodes, for comparing different versions of a knowledge hierarchy, modeled as a generic directed acyclic graph. We present theoretical analysis to depict that the proposed measure accurately captures the salient properties of taxonomic hierarchies, assesses changes in the ordering of nodes, along with the logical subsumption of relationships among concepts. We also present a linear time variant of the measure, and show that our measures, unlike previous approaches, are tunable to cater to diverse application needs. We further show that our measure provides interpretability, thereby identifying the key structural and logical difference in the hierarchies. Experiments on a real DBpedia and biological knowledge hierarchy showcase that our measures accurately capture structural similarity, while providing enhanced scalability and tunability. Also, we demonstrate that the temporal evolution of different subgraphs in this knowledge hierarchy, as captured purely by our structural measure, corresponds well with the known disruptions in the related subject areas.",Automated assessment of knowledge hierarchy evolution: comparing directed acyclic graphs,https://link.springer.com/article/10.1007/s10791-018-9345-y,https://www.researchgate.net/publication/329716534_Automated_assessment_of_knowledge_hierarchy_evolution_comparing_directed_acyclic_graphs,422,https://github.com/guruprasadnk7/DAGSimilarityKatz,1,Machine Translation,,"knowledge graph completion,knowledge graphs,question answering"
,,"    Tensor factorization has become an increasingly popular approach to knowledgegraph completion(KGC), which is the task of automatically predicting missingfacts in a knowledge graph. However, even with a simple model likeCANDECOMP/PARAFAC(CP) tensor decomposition, KGC on existing knowledge graphs isimpractical in resource-limited environments, as a large amount of memory isrequired to store parameters represented as 32-bit or 64-bit floating pointnumbers.This limitation is expected to become more stringent as existingknowledge graphs, which are already huge, keep steadily growing in scale. Toreduce the memory requirement, we present a method for binarizing theparameters of the CP tensor decomposition by introducing a quantizationfunction to the optimization problem. This method replaces floatingpoint-valued parameters with binary ones after training, which drasticallyreduces the model size at run time. We investigate the trade-off between thequality and size of tensor factorization models for several KGC benchmarkdatasets. In our experiments, the proposed method successfully reduced themodel size by more than an order of magnitude while maintaining the taskperformance. Moreover, a fast score computation technique can be developed withbitwise operations.",Binarized Knowledge Graph Embeddings,https://arxiv.org/abs/1902.02970v1,https://arxiv.org/pdf/1902.02970v1.pdf,422,"https://github.com/KokiKishimoto/cp_decomposition,https://github.com/KokiKishimoto/cp_decomposition_alpha","6,6",Machine Translation,,"knowledge graph completion,knowledge graph embeddings,knowledge graphs,quantization"
,,"    We present a framework for generating natural language description fromstructured data such as tables. Motivated by the need to approach this problemin a manner that is scalable and easily adaptable to newer domains, unlikeexisting related systems, our system does not require parallel data; it ratherrelies on monolingual corpora and basic NLP tools which are easily accessible.The system employs a 3-staged pipeline that: (i) converts entries in thestructured data to canonical form, (ii) generates simple sentences for eachatomic entry in the canonicalized representation, and (iii) combines thesentences to produce a coherent, fluent and adequate paragraph descriptionthrough sentence compounding and co-reference replacement modules. Experimentson a benchmark mixed-domain dataset curated for paragraph description fromtables reveals the superiority of our system over existing data-to-textapproaches. We also demonstrate the robustness of our system in accepting otherdata types such as Knowledge-Graphs and Key-Value dictionaries.",Scalable Micro-planned Generation of Discourse from Structured Data,https://arxiv.org/abs/1810.02889v1,https://arxiv.org/pdf/1810.02889v1.pdf,422,https://github.com/parajain/structscribe,1,Machine Translation,,knowledge graphs
,,"    There is a great diversity of clustering and community detection algorithms, which are key components of many data analysis and exploration systems. To the best of our knowledge, however, there does not exist yet any uniform benchmarking framework, which is publicly available and suitable for the parallel benchmarking of diverse clustering algorithms on a wide range of synthetic and real-world datasets.In this paper, we introduce Clubmark, a new extensible framework that aims to fill this gap by providing a parallel isolation benchmarking platform for clustering algorithms and their evaluation on NUMA servers. Clubmark allows for fine-grained control over various execution variables (timeouts, memory consumption, CPU affinity and cache policy) and supports the evaluation of a wide range of clustering algorithms including multi-level, hierarchical and overlapping clustering techniques on both weighted and unweighted input networks with built-in evaluation of several extrinsic and intrinsic measures. Our framework is open-source and provides a consistent and systematic way to execute, evaluate and profile clustering techniques considering a number of aspects that are often missing in state-of-the-art frameworks and benchmarking systems.",Clubmark: a Parallel Isolation Framework for Benchmarking and Profiling Clustering Algorithms on NUMA Architectures,https://arxiv.org/abs/1902.00475,https://arxiv.org/pdf/1902.00475,422,https://github.com/eXascaleInfolab/clubmark,6,Machine Translation,,"clustering algorithms evaluation,community detection"
,,"    While conversing with chatbots, humans typically tend to ask many questions,a significant portion of which can be answered by referring to large-scaleknowledge graphs (KG). While Question Answering (QA) and dialog systems havebeen studied independently, there is a need to study them closely to evaluatesuch real-world scenarios faced by bots involving both these tasks.Towardsthis end, we introduce the task of Complex Sequential QA which combines the twotasks of (i) answering factual questions through complex inferencing over arealistic-sized KG of millions of entities, and (ii) learning to conversethrough a series of coherently linked QA pairs. Through a labor intensivesemi-automatic process, involving in-house and crowdsourced workers, we createda dataset containing around 200K dialogs with a total of 1.6M turns. Further,unlike existing large scale QA datasets which contain simple questions that canbe answered from a single tuple, the questions in our dialogs require a largersubgraph of the KG. Specifically, our dataset has questions which requirelogical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions,(ii) use conversation context to resolve coreferences and ellipsis inutterances, (iii) ask for clarifications for ambiguous queries, and finally(iv) retrieve relevant subgraphs of the KG to answer such questions. However,our experiments with a combination of state of the art dialog and QA modelsshow that they clearly do not achieve the above objectives and are inadequatefor dealing with such complex real world settings. We believe that this newdataset coupled with the limitations of existing models as reported in thispaper should encourage further research in Complex Sequential QA.",Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph,https://arxiv.org/abs/1801.10314v2,https://arxiv.org/pdf/1801.10314v2.pdf,422,https://github.com/mali-git/CSQA_Implementation,2,Machine Translation,,"knowledge graphs,question answering"
,,"    While conversing with chatbots, humans typically tend to ask many questions,a significant portion of which can be answered by referring to large-scaleknowledge graphs (KG). While Question Answering (QA) and dialog systems havebeen studied independently, there is a need to study them closely to evaluatesuch real-world scenarios faced by bots involving both these tasks.Towardsthis end, we introduce the task of Complex Sequential QA which combines the twotasks of (i) answering factual questions through complex inferencing over arealistic-sized KG of millions of entities, and (ii) learning to conversethrough a series of coherently linked QA pairs. Through a labor intensivesemi-automatic process, involving in-house and crowdsourced workers, we createda dataset containing around 200K dialogs with a total of 1.6M turns. Further,unlike existing large scale QA datasets which contain simple questions that canbe answered from a single tuple, the questions in our dialogs require a largersubgraph of the KG. Specifically, our dataset has questions which requirelogical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions,(ii) use conversation context to resolve coreferences and ellipsis inutterances, (iii) ask for clarifications for ambiguous queries, and finally(iv) retrieve relevant subgraphs of the KG to answer such questions. However,our experiments with a combination of state of the art dialog and QA modelsshow that they clearly do not achieve the above objectives and are inadequatefor dealing with such complex real world settings. We believe that this newdataset coupled with the limitations of existing models as reported in thispaper should encourage further research in Complex Sequential QA.",Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph,https://arxiv.org/abs/1801.10314v2,https://arxiv.org/pdf/1801.10314v2.pdf,422,https://github.com/mali-git/CSQA_Implementation,2,Machine Translation,,"knowledge graphs,question answering"
,,"    We consider the problem of zero-shot recognition: learning a visualclassifier for a category with zero training examples, just using the wordembedding of the category and its relationship to other categories, whichvisual data are provided. The key to dealing with the unfamiliar or novelcategory is to transfer knowledge obtained from familiar classes to describethe unfamiliar class.In this paper, we build upon the recently introducedGraph Convolutional Network (GCN) and propose an approach that uses bothsemantic embeddings and the categorical relationships to predict theclassifiers. Given a learned knowledge graph (KG), our approach takes as inputsemantic embeddings for each node (representing visual category). After aseries of graph convolutions, we predict the visual classifier for eachcategory. During training, the visual classifiers for a few categories aregiven to learn the GCN parameters. At test time, these filters are used topredict the visual classifiers of unseen categories. We show that our approachis robust to noise in the KG. More importantly, our approach providessignificant improvement in performance compared to the current state-of-the-artresults (from 2 ~ 3% on some metrics to whopping 20% on a few).",Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs,https://arxiv.org/abs/1803.08035v2,https://arxiv.org/pdf/1803.08035v2.pdf,422,https://github.com/MARMOTatZJU/ZSLPR-TIANCHI,2,Machine Translation,,"knowledge graphs,zero shot learning"
,,"    Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems.However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.",Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems,https://arxiv.org/abs/1905.04413v3,https://arxiv.org/pdf/1905.04413v3.pdf,422,https://github.com/hwwang55/KGNN-LS,6,Machine Translation,,"feature engineering,graph neural network,knowledge graphs,recommendation systems"
,,"    Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions.The new framework results in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduce training time at a cost of an additional approximation to the variational lower bound. We introduce two models from this highly scalable probabilistic framework, namely the Latent Information and Latent Fact models, for reasoning over knowledge graph-based representations. Our Latent Information and Latent Fact models improve upon baseline performance under certain conditions. We use the learnt embedding variance to estimate predictive uncertainty during link prediction, and discuss the quality of these learnt uncertainty estimates. Our source code and datasets are publicly available online at https://github.com/alexanderimanicowenrivers/Neural-Variational-Knowledge-Graphs.",Neural Variational Inference For Estimating Uncertainty in Knowledge Graph Embeddings,https://arxiv.org/abs/1906.04985v1,https://arxiv.org/pdf/1906.04985v1.pdf,422,https://github.com/alexanderimanicowenrivers/Neural-Variational-Knowledge-Graphs,6,Machine Translation,,"knowledge graph embeddings,knowledge graphs,link prediction"
,,"    Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions.The new framework results in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduce training time at a cost of an additional approximation to the variational lower bound. We introduce two models from this highly scalable probabilistic framework, namely the Latent Information and Latent Fact models, for reasoning over knowledge graph-based representations. Our Latent Information and Latent Fact models improve upon baseline performance under certain conditions. We use the learnt embedding variance to estimate predictive uncertainty during link prediction, and discuss the quality of these learnt uncertainty estimates. Our source code and datasets are publicly available online at https://github.com/alexanderimanicowenrivers/Neural-Variational-Knowledge-Graphs.",Neural Variational Inference For Estimating Uncertainty in Knowledge Graph Embeddings,https://arxiv.org/abs/1906.04985v1,https://arxiv.org/pdf/1906.04985v1.pdf,422,https://github.com/alexanderimanicowenrivers/Neural-Variational-Knowledge-Graphs,6,Machine Translation,,"knowledge graph embeddings,knowledge graphs,link prediction"
,,"    While large-scale knowledge graphs provide vast amounts of structured factsabout entities, a short textual description can often be useful to succinctlycharacterize an entity and its type. Unfortunately, many knowledge graphentities lack such textual descriptions.In this paper, we introduce a dynamicmemory-based network that generates a short open vocabulary description of anentity by jointly leveraging induced fact embeddings as well as the dynamiccontext of the generated sequence of words. We demonstrate the ability of ourarchitecture to discern relevant information for more accurate generation oftype description by pitting the system against several strong baselines.",Generating Fine-Grained Open Vocabulary Entity Type Descriptions,https://arxiv.org/abs/1805.10564v1,https://arxiv.org/pdf/1805.10564v1.pdf,422,https://github.com/kingsaint/Open-vocabulary-entity-type-description,8,Machine Translation,,knowledge graphs
,,"    Question answering over knowledge graphs is an important problem of interest both commercially and academically. There is substantial interest in the class of natural language questions that can be answered via the lookup of a single fact, driven by the availability of the popular SimpleQuestions dataset.The problem with this dataset, however, is that answer triples are provided from Freebase, which has been defunct for several years. As a result, it is difficult to build {``}real-world{''} question answering systems that are operationally deployable. Furthermore, a defunct knowledge graph means that much of the infrastructure for querying, browsing, and manipulating triples no longer exists. To address this problem, we present SimpleDBpediaQA, a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from Freebase to DBpedia. Although this mapping is conceptually straightforward, there are a number of nuances that make the task non-trivial, owing to the different conceptual organizations of the two knowledge graphs. To lay the foundation for future research using this dataset, we leverage recent work to provide simple yet strong baselines with and without neural networks.",Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia,https://www.aclweb.org/anthology/papers/C/C18/C18-1178/,https://www.aclweb.org/anthology/C18-1178,422,https://github.com/castorini/SimpleDBpediaQA,11,Machine Translation,,"knowledge graphs,question answering,transfer learning"
,,"    There has always been criticism for using $n$-gram based similarity metrics,such as BLEU, NIST, etc, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being usedfor evaluating the performance of systems which automatically generatequestions from documents, knowledge graphs, images, etc.Given the risinginterest in such automatic question generation (AQG) systems, it is importantto objectively examine whether these metrics are suitable for this task. Inparticular, it is important to verify whether such metrics used for evaluatingAQG systems focus on answerability of the generated question by preferringquestions which contain all relevant information such as question type(Wh-types), entities, relations, etc. In this work, we show that currentautomatic evaluation metrics based on $n$-gram similarity do not alwayscorrelate well with human judgments about answerability of a question. Toalleviate this problem and as a first step towards better evaluation metricsfor AQG, we introduce a scoring function to capture answerability and show thatwhen this scoring function is integrated with existing metrics, they correlatesignificantly better with human judgments. The scripts and data developed as apart of this work are made publicly available athttps://github.com/PrekshaNema25/Answerability-Metric",Towards a Better Metric for Evaluating Question Generation Systems,https://arxiv.org/abs/1808.10192v2,https://arxiv.org/pdf/1808.10192v2.pdf,422,https://github.com/PrekshaNema25/Answerability-Metric,11,Machine Translation,,"knowledge graphs,question generation"
,,"    Representation learning provides new and powerful graph analytical approaches and tools for the highly valued data science challenge of mining knowledge graphs. Since previous graph analytical methods have mostly focused on homogeneous graphs, an important current challenge is extending this methodology for richly heterogeneous graphs and knowledge domains.The biomedical sciences are such a domain, reflecting the complexity of biology, with entities such as genes, proteins, drugs, diseases, and phenotypes, and relationships such as gene co-expression, biochemical regulation, and biomolecular inhibition or activation. Therefore, the semantics of edges and nodes are critical for representation learning and knowledge discovery in real world biomedical problems. In this paper, we propose the edge2vec model, which represents graphs considering edge semantics. An edge-type transition matrix is trained by an Expectation-Maximization approach, and a stochastic gradient descent model is employed to learn node embedding on a heterogeneous graph via the trained transition matrix. edge2vec is validated on three biomedical domain tasks: biomedical entity classification, compound-gene bioactivity prediction, and biomedical information retrieval. Results show that by considering edge-types into node embedding learning in heterogeneous graphs, \textbf{edge2vec}\ significantly outperforms state-of-the-art models on all three tasks. We propose this method for its added value relative to existing graph analytical methodology, and in the real world context of biomedical knowledge discovery applicability.",edge2vec: Representation learning using edge semantics for biomedical knowledge discovery,https://arxiv.org/abs/1809.02269v3,https://arxiv.org/pdf/1809.02269v3.pdf,422,https://github.com/RoyZhengGao/edge2vec,11,Machine Translation,,"information retrieval,knowledge graphs,representation learning"
,,"    Knowledge graphs, on top of entities and their relationships, contain otherimportant elements: literals. Literals encode interesting properties (e.g. theheight) of entities that are not captured by links between entities alone.Mostof the existing work on embedding (or latent feature) based knowledge graphanalysis focuses mainly on the relations between entities. In this work, westudy the effect of incorporating literal information into existing linkprediction methods. Our approach, which we name LiteralE, is an extension thatcan be plugged into existing latent feature methods. LiteralE merges entityembeddings with their literal information using a learnable, parametrizedfunction, such as a simple linear or nonlinear transformation, or a multilayerneural network. We extend several popular embedding models based on LiteralEand evaluate their performance on the task of link prediction. Despite itssimplicity, LiteralE proves to be an effective way to incorporate literalinformation into existing embedding based methods, improving their performanceon different standard datasets, which we augmented with their literals andprovide as testbed for further research.",Incorporating Literals into Knowledge Graph Embeddings,https://arxiv.org/abs/1802.00934v2,https://arxiv.org/pdf/1802.00934v2.pdf,422,https://github.com/SmartDataAnalytics/LiteralE,18,Machine Translation,,"knowledge graph embeddings,knowledge graphs,link prediction"
,,"    Due to the fact much of today's data can be represented as graphs, there has been a demand for generalizing neural network models for graph data. One recent direction that has shown fruitful results, and therefore growing interest, is the usage of graph convolutional neural networks (GCNs).They have been shown to provide a significant improvement on a wide range of tasks in network analysis, one of which being node representation learning. The task of learning low-dimensional node representations has shown to increase performance on a plethora of other tasks from link prediction and node classification, to community detection and visualization. Simultaneously, signed networks (or graphs having both positive and negative links) have become ubiquitous with the growing popularity of social media. However, since previous GCN models have primarily focused on unsigned networks (or graphs consisting of only positive links), it is unclear how they could be applied to signed networks due to the challenges presented by negative links. The primary challenges are based on negative links having not only a different semantic meaning as compared to positive links, but their principles are inherently different and they form complex relations with positive links. Therefore we propose a dedicated and principled effort that utilizes balance theory to correctly aggregate and propagate the information across layers of a signed GCN model. We perform empirical experiments comparing our proposed signed GCN against state-of-the-art baselines for learning node representations in signed networks. More specifically, our experiments are performed on four real-world datasets for the classical link sign prediction problem that is commonly used as the benchmark for signed network embeddings algorithms.",Signed Graph Convolutional Network,https://arxiv.org/abs/1808.06354,https://arxiv.org/pdf/1808.06354.pdf,422,https://github.com/benedekrozemberczki/SGCN,84,Machine Translation,,"community detection,link prediction,link sign prediction,node classification,representation learning"
,,"    Due to the fact much of today's data can be represented as graphs, there has been a demand for generalizing neural network models for graph data. One recent direction that has shown fruitful results, and therefore growing interest, is the usage of graph convolutional neural networks (GCNs).They have been shown to provide a significant improvement on a wide range of tasks in network analysis, one of which being node representation learning. The task of learning low-dimensional node representations has shown to increase performance on a plethora of other tasks from link prediction and node classification, to community detection and visualization. Simultaneously, signed networks (or graphs having both positive and negative links) have become ubiquitous with the growing popularity of social media. However, since previous GCN models have primarily focused on unsigned networks (or graphs consisting of only positive links), it is unclear how they could be applied to signed networks due to the challenges presented by negative links. The primary challenges are based on negative links having not only a different semantic meaning as compared to positive links, but their principles are inherently different and they form complex relations with positive links. Therefore we propose a dedicated and principled effort that utilizes balance theory to correctly aggregate and propagate the information across layers of a signed GCN model. We perform empirical experiments comparing our proposed signed GCN against state-of-the-art baselines for learning node representations in signed networks. More specifically, our experiments are performed on four real-world datasets for the classical link sign prediction problem that is commonly used as the benchmark for signed network embeddings algorithms.",Signed Graph Convolutional Network,https://arxiv.org/abs/1808.06354,https://arxiv.org/pdf/1808.06354.pdf,422,https://github.com/benedekrozemberczki/SGCN,84,Machine Translation,,"community detection,link prediction,link sign prediction,node classification,representation learning"
,,"    Embedding knowledge graphs (KGs) into continuous vector spaces is a focus ofcurrent research. Combining such an embedding model with logic rules hasrecently attracted increasing attention.Most previous attempts made a one-timeinjection of logic rules, ignoring the interactive nature between embeddinglearning and logical inference. And they focused only on hard rules, whichalways hold with no exception and usually require extensive manual effort tocreate or validate. In this paper, we propose Rule-Guided Embedding (RUGE), anovel paradigm of KG embedding with iterative guidance from soft rules. RUGEenables an embedding model to learn simultaneously from 1) labeled triples thathave been directly observed in a given KG, 2) unlabeled triples whose labelsare going to be predicted iteratively, and 3) soft rules with variousconfidence levels extracted automatically from the KG. In the learning process,RUGE iteratively queries rules to obtain soft labels for unlabeled triples, andintegrates such newly labeled triples to update the embedding model. Throughthis iterative procedure, knowledge embodied in logic rules may be bettertransferred into the learned embeddings. We evaluate RUGE in link prediction onFreebase and YAGO. Experimental results show that: 1) with rule knowledgeinjected iteratively, RUGE achieves significant and consistent improvementsover state-of-the-art baselines; and 2) despite their uncertainties,automatically extracted soft rules are highly beneficial to KG embedding, eventhose with moderate confidence levels. The code and data used for this papercan be obtained from https://github.com/iieir-km/RUGE.",Knowledge Graph Embedding with Iterative Guidance from Soft Rules,https://arxiv.org/abs/1711.11231v1,https://arxiv.org/pdf/1711.11231v1.pdf,422,https://github.com/iieir-km/RUGE,19,Machine Translation,,"graph embedding,knowledge graph embedding,knowledge graphs,link prediction"
,,"    A limitation of the Graph Convolutional Networks (GCNs) is that it assumes at a particular $l^{th}$ layer of the neural network model only the $l^{th}$ order neighbourhood nodes of a social network are influential. Furthermore, the GCN has been evaluated on citation and knowledge graphs, but not extensively on friendship-based social graphs.The drawback associated with the dependencies between layers and the order of node neighbourhood for the GCN can be more prevalent for friendship-based graphs. The evaluation of the full potential of the GCN on friendship-based social network requires openly available datasets in larger quantities. However, most available social network datasets are not complete. Also, the majority of the available social network datasets do not contain both the features and ground truth labels. In this work, firstly, we provide a guideline on simulating dynamic social networks, with ground truth labels and features, both coupled with the topology. Secondly, we introduce an open-source Python-based simulation library. We argue that the topology of the network is driven by a set of latent variables, termed as the social DNA (sDNA). We consider the sDNA as labels for the nodes. Finally, by evaluating on our simulated datasets, we propose four new variants of the GCN, mainly to overcome the limitation of dependency between the order of node-neighbourhood and a particular layer of the model. We then evaluate the performance of all the models and our results show that on 27 out of the 30 simulated datasets our proposed GCN variants outperform the original model.",Simulation and Augmentation of Social Networks for Building Deep Learning Models,https://arxiv.org/abs/1905.09087v2,https://arxiv.org/pdf/1905.09087v2.pdf,422,https://github.com/AkandaAshraf/VirtualSoc,8,Machine Translation,,knowledge graphs
,,"    The volume and velocity of information that gets generated online limitscurrent journalistic practices to fact-check claims at the same rate. Computational approaches for fact checking may be the key to help mitigate therisks of massive misinformation spread.Such approaches can be designed to notonly be scalable and effective at assessing veracity of dubious claims, butalso to boost a human fact checker's productivity by surfacing relevant factsand patterns to aid their analysis. To this end, we present a novel,unsupervised network-flow based approach to determine the truthfulness of astatement of fact expressed in the form of a (subject, predicate, object)triple. We view a knowledge graph of background information about real-worldentities as a flow network, and knowledge as a fluid, abstract commodity. Weshow that computational fact checking of such a triple then amounts to findinga ""knowledge stream"" that emanates from the subject node and flows toward theobject node through paths connecting them. Evaluation on a range of real-worldand hand-crafted datasets of facts related to entertainment, business, sports,geography and more reveals that this network-flow model can be very effectivein discerning true statements from false ones, outperforming existingalgorithms on many test cases. Moreover, the model is expressive in its abilityto automatically discover several useful path patterns and surface relevantfacts that may help a human fact checker corroborate or refute a claim.",Finding Streams in Knowledge Graphs to Support Fact Checking,https://arxiv.org/abs/1708.07239v1,https://arxiv.org/pdf/1708.07239v1.pdf,422,https://github.com/shiralkarprashant/knowledgestream,20,Machine Translation,,knowledge graphs
,,"    Conversational systems have become increasingly popular as a way for humansto interact with computers. To be able to provide intelligent responses,conversational systems must correctly model the structure and semantics of aconversation.We introduce the task of measuring semantic (in)coherence in aconversation with respect to background knowledge, which relies on theidentification of semantic relations between concepts introduced during aconversation. We propose and evaluate graph-based and machine learning-basedapproaches for measuring semantic coherence using knowledge graphs, theirvector space embeddings and word embedding models, as sources of backgroundknowledge. We demonstrate how these approaches are able to uncover differentcoherence patterns in conversations on the Ubuntu Dialogue Corpus.",Measuring Semantic Coherence of a Conversation,https://arxiv.org/abs/1806.06411v1,https://arxiv.org/pdf/1806.06411v1.pdf,422,"https://github.com/vendi12/semantic_coherence,https://github.com/svakulenk0/semantic_coherence","8,8",Machine Translation,,knowledge graphs
,,"    Motivation: Biological data and knowledge bases increasingly rely on SemanticWeb technologies and the use of knowledge graphs for data integration,retrieval and federated queries. In the past years, feature learning methodsthat are applicable to graph-structured data are becoming available, but havenot yet widely been applied and evaluated on structured biological knowledge.Results: We develop a novel method for feature learning on biological knowledgegraphs. Our method combines symbolic methods, in particular knowledgerepresentation using symbolic logic and automated reasoning, with neuralnetworks to generate embeddings of nodes that encode for related informationwithin knowledge graphs. Through the use of symbolic logic, these embeddingscontain both explicit and implicit information. We apply these embeddings tothe prediction of edges in the knowledge graph representing problems offunction prediction, finding candidate genes of diseases, protein-proteininteractions, or drug target relations, and demonstrate performance thatmatches and sometimes outperforms traditional approaches based on manuallycrafted features. Our method can be applied to any biological knowledge graph,and will thereby open up the increasing amount of Semantic Web based knowledgebases in biology to use in machine learning and data analytics. Availabilityand Implementation:https://github.com/bio-ontology-research-group/walking-rdf-and-owl Contact:",Neuro-symbolic representation learning on biological knowledge graphs,https://arxiv.org/abs/1612.04256v1,https://arxiv.org/pdf/1612.04256v1.pdf,422,https://github.com/bio-ontology-research-group/walking-rdf-and-owl,21,Machine Translation,,"knowledge graphs,representation learning"
,,"    Unlike machines, humans learn through rapid, abstract model-building. Therole of a teacher is not simply to hammer home right or wrong answers, butrather to provide intuitive comments, comparisons, and explanations to a pupil.This is what the Learning Under Privileged Information (LUPI) paradigmendeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional NeuralNetworks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use aheteroscedastic dropout (i.e. dropout with a varying variance) and make thevariance of the dropout a function of privileged information. Intuitively, thiscorresponds to using the privileged information to control the uncertainty ofthe model output. We perform experiments using CNNs and RNNs for the tasks ofimage classification and machine translation. Our method significantlyincreases the sample efficiency during learning, resulting in higher accuracywith a large margin when the number of training examples is limited. We alsotheoretically justify the gains in sample efficiency by providing ageneralization error bound decreasing with $O(\frac{1}{n})$, where $n$ is thenumber of training examples, in an oracle case.",Deep Learning under Privileged Information Using Heteroscedastic Dropout,https://arxiv.org/abs/1805.11614v1,https://arxiv.org/pdf/1805.11614v1.pdf,422,https://github.com/johnwlambert/dlupi-heteroscedastic-dropout,18,Machine Translation,,"image classification,machine translation"
,,"    Recent advances in statistical machine translation via the adoption of neuralsequence-to-sequence models empower the end-to-end system to achievestate-of-the-art in many WMT benchmarks. The performance of such machinetranslation (MT) system is usually evaluated by automatic metric BLEU when thegolden references are provided for validation.However, for model inference orproduction deployment, the golden references are prohibitively available orrequire expensive human annotation with bilingual expertise. In order toaddress the issue of quality evaluation (QE) without reference, we propose ageneral framework for automatic evaluation of translation output for most WMTquality evaluation tasks. We first build a conditional target language modelwith a novel bidirectional transformer, named neural bilingual expert model,which is pre-trained on large parallel corpora for feature extraction. For QEinference, the bilingual expert model can simultaneously produce the jointlatent representation between the source and the translation, and real-valuedmeasurements of possible erroneous tokens based on the prior knowledge learnedfrom parallel data. Subsequently, the features will further be fed into asimple Bi-LSTM predictive model for quality evaluation. The experimentalresults show that our approach achieves the state-of-the-art performance in thequality estimation track of WMT 2017/2018.","""Bilingual Expert"" Can Find Translation Errors",https://arxiv.org/abs/1807.09433v3,https://arxiv.org/pdf/1807.09433v3.pdf,422,https://github.com/lovecambi/qebrain,18,Machine Translation,,"language modelling,machine translation"
,,"    Knowledge graphs are graphical representations of large databases of facts, which typically suffer from incompleteness. Inferring missing relations (links) between entities (nodes) is the task of link prediction.A recent state-of-the-art approach to link prediction, ConvE, implements a convolutional neural network to extract features from concatenated subject and relation vectors. Whilst results are impressive, the method is unintuitive and poorly understood. We propose a hypernetwork architecture that generates simplified relation-specific convolutional filters that (i) outperforms ConvE and all previous approaches across standard datasets; and (ii) can be framed as tensor factorization and thus set within a well established family of factorization models for link prediction. We thus demonstrate that convolution simply offers a convenient computational means of introducing sparsity and parameter tying to find an effective trade-off between non-linear expressiveness and the number of parameters to learn.",Hypernetwork Knowledge Graph Embeddings,https://arxiv.org/abs/1808.07018v4,https://arxiv.org/pdf/1808.07018v4.pdf,422,https://github.com/ibalazevic/HypER,18,Machine Translation,,"knowledge graph embeddings,knowledge graphs,link prediction"
,,"    In multilingual neural machine translation, it has been shown that sharing asingle translation model between multiple languages can achieve competitiveperformance, sometimes even leading to performance gains over bilinguallytrained models. However, these improvements are not uniform; often multilingualparameter sharing results in a decrease in accuracy due to translation modelsnot being able to accommodate different languages in their limited parameterspace.In this work, we examine parameter sharing techniques that strike ahappy medium between full sharing and individual training, specificallyfocusing on the self-attentional Transformer model. We find that the fullparameter sharing approach leads to increases in BLEU scores mainly when thetarget languages are from a similar language family. However, even in the casewhere target languages are from different families where full parameter sharingleads to a noticeable drop in BLEU scores, our proposed methods for partialsharing of parameters can lead to substantial improvements in translationaccuracy.",Parameter Sharing Methods for Multilingual Self-Attentional Translation Models,https://arxiv.org/abs/1809.00252v2,https://arxiv.org/pdf/1809.00252v2.pdf,422,https://github.com/DevSinghSachan/multilingual_nmt,18,Machine Translation,,machine translation
,,"    Statistical language modeling techniques have successfully been applied tosource code, yielding a variety of new software development tools, such astools for code suggestion and improving readability. A major issue with thesetechniques is that code introduces new vocabulary at a far higher rate thannatural language, as new identifier names proliferate.But traditional languagemodels limit the vocabulary to a fixed set of common words. For code, thisstrong assumption has been shown to have a significant negative effect onpredictive performance. But the open vocabulary version of the neural networklanguage models for code have not been introduced in the literature. We presenta new open-vocabulary neural language model for code that is not limited to afixed vocabulary of identifier names. We employ a segmentation into subwordunits, subsequences of tokens chosen based on a compression criterion,following previous work in machine translation. Our network achieves best inclass performance, outperforming even the state-of-the-art methods ofHellendoorn and Devanbu that are designed specifically to model code. Furthermore, we present a simple method for dynamically adapting the model to anew test project, resulting in increased performance. We showcase ourmethodology on code corpora in three different languages of over a billiontokens each, hundreds of times larger than in previous work. To our knowledge,this is the largest neural language model for code that has been reported.",Maybe Deep Neural Networks are the Best Choice for Modeling Source Code,https://arxiv.org/abs/1903.05734v1,https://arxiv.org/pdf/1903.05734v1.pdf,422,https://github.com/mast-group/OpenVocabCodeNLM,19,Machine Translation,,"language modelling,machine translation"
,,"    We present LemmaTag, a featureless neural network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the network, predicting tag subcategories, and using the tagger output as an input to the lemmatizer.We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both part-of-speech tagging and lemmatization in Czech, German, and Arabic.",LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs,https://www.aclweb.org/anthology/papers/D/D18/D18-1532/,https://www.aclweb.org/anthology/D18-1532,422,https://github.com/hyperparticle/LemmaTag,20,Machine Translation,,"lemmatization,machine translation,part of speech tagging,semantic role labeling,sentiment analysis"
,,"    We present LemmaTag, a featureless neural network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the network, predicting tag subcategories, and using the tagger output as an input to the lemmatizer.We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both part-of-speech tagging and lemmatization in Czech, German, and Arabic.",LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs,https://www.aclweb.org/anthology/papers/D/D18/D18-1532/,https://www.aclweb.org/anthology/D18-1532,422,https://github.com/hyperparticle/LemmaTag,20,Machine Translation,,"lemmatization,machine translation,part of speech tagging,semantic role labeling,sentiment analysis"
,,"    Recent advances in Neural Machine Translation (NMT) show that addingsyntactic information to NMT systems can improve the quality of theirtranslations. Most existing work utilizes some specific types oflinguistically-inspired tree structures, like constituency and dependency parsetrees.This is often done via a standard RNN decoder that operates on alinearized target tree structure. However, it is an open question of whatspecific linguistic formalism, if any, is the best structural representationfor NMT. In this paper, we (1) propose an NMT model that can naturally generatethe topology of an arbitrary tree structure on the target side, and (2)experiment with various target tree structures. Our experiments show thesurprising result that our model delivers the best improvements with balancedbinary trees constructed without any linguistic knowledge; this modeloutperforms standard seq2seq models by up to 2.1 BLEU points, and other methodsfor incorporating target-side syntax by up to 0.7 BLEU.",A Tree-based Decoder for Neural Machine Translation,https://arxiv.org/abs/1808.09374v1,https://arxiv.org/pdf/1808.09374v1.pdf,422,https://github.com/cindyxinyiwang/TrDec_pytorch,21,Machine Translation,,machine translation
,,"    Recent advances in Neural Machine Translation (NMT) show that addingsyntactic information to NMT systems can improve the quality of theirtranslations. Most existing work utilizes some specific types oflinguistically-inspired tree structures, like constituency and dependency parsetrees.This is often done via a standard RNN decoder that operates on alinearized target tree structure. However, it is an open question of whatspecific linguistic formalism, if any, is the best structural representationfor NMT. In this paper, we (1) propose an NMT model that can naturally generatethe topology of an arbitrary tree structure on the target side, and (2)experiment with various target tree structures. Our experiments show thesurprising result that our model delivers the best improvements with balancedbinary trees constructed without any linguistic knowledge; this modeloutperforms standard seq2seq models by up to 2.1 BLEU points, and other methodsfor incorporating target-side syntax by up to 0.7 BLEU.",A Tree-based Decoder for Neural Machine Translation,https://arxiv.org/abs/1808.09374v1,https://arxiv.org/pdf/1808.09374v1.pdf,422,https://github.com/cindyxinyiwang/TrDec_pytorch,21,Machine Translation,,machine translation
,,"    Grammatical error correction (GEC) systems deployed in language learning environments are expected to accurately correct errors in learners{'} writing. However, in practice, they often produce spurious corrections and fail to correct many errors, thereby misleading learners.This necessitates the estimation of the quality of output sentences produced by GEC systems so that instructors can selectively intervene and re-correct the sentences which are poorly corrected by the system and ensure that learners get accurate feedback. We propose the first neural approach to automatic quality estimation of GEC output sentences that does not employ any hand-crafted features. Our system is trained in a supervised manner on learner sentences and corresponding GEC system outputs with quality score labels computed using human-annotated references. Our neural quality estimation models for GEC show significant improvements over a strong feature-based baseline. We also show that a state-of-the-art GEC system can be improved when quality scores are used as features for re-ranking the N-best candidates.",Neural Quality Estimation of Grammatical Error Correction,https://www.aclweb.org/anthology/papers/D/D18/D18-1274/,https://www.aclweb.org/anthology/D18-1274,422,https://github.com/nusnlp/neuqe,20,Machine Translation,,"grammatical error correction,machine translation"
,,"    Inferring new facts from existing knowledge graphs (KG) with explainable reasoning processes is a significant problem and has received much attention recently. However, few studies have focused on relation types unseen in the original KG, given only one or a few instances for training.To bridge this gap, we propose CogKR for one-shot KG reasoning. The one-shot relational learning problem is tackled through two modules: the summary module summarizes the underlying relationship of the given instances, based on which the reasoning module infers the correct answers. Motivated by the dual process theory in cognitive science, in the reasoning module, a cognitive graph is built by iteratively coordinating retrieval (System 1, collecting relevant evidence intuitively) and reasoning (System 2, conducting relational reasoning over collected information). The structural information offered by the cognitive graph enables our model to aggregate pieces of evidence from multiple reasoning paths and explain the reasoning process graphically. Experiments show that CogKR substantially outperforms previous state-of-the-art models on one-shot KG reasoning benchmarks, with relative improvements of 24.3%-29.7% on MRR. The source code is available at https://github.com/THUDM/CogKR.",Cognitive Knowledge Graph Reasoning for One-shot Relational Learning,https://arxiv.org/abs/1906.05489v1,https://arxiv.org/pdf/1906.05489v1.pdf,422,https://github.com/THUDM/CogKR,8,Machine Translation,,"knowledge graphs,relational reasoning"
,,"    Knowledge graphs are large, useful, but incomplete knowledge repositories. They encode knowledge through entities and relations which define each otherthrough the connective structure of the graph.This has inspired methods forthe joint embedding of entities and relations in continuous low-dimensionalvector spaces, that can be used to induce new edges in the graph, i.e., linkprediction in knowledge graphs. Learning these representations relies oncontrasting positive instances with negative ones. Knowledge graphs includeonly positive relation instances, leaving the door open for a variety ofmethods for selecting negative examples. In this paper we present an empiricalstudy on the impact of negative sampling on the learned embeddings, assessedthrough the task of link prediction. We use state-of-the-art knowledge graphembeddings -- \rescal , TransE, DistMult and ComplEX -- and evaluate onbenchmark datasets -- FB15k and WN18. We compare well known methods fornegative sampling and additionally propose embedding based sampling methods. Wenote a marked difference in the impact of these sampling methods on the twodatasets, with the ""traditional"" corrupting positives method leading to bestresults on WN18, while embedding based methods benefiting the task on FB15k.",Analysis of the Impact of Negative Sampling on Link Prediction in Knowledge Graphs,https://arxiv.org/abs/1708.06816v2,https://arxiv.org/pdf/1708.06816v2.pdf,422,https://github.com/bhushank/kge-rl,9,Machine Translation,,"knowledge graph embeddings,knowledge graphs,link prediction"
,,"    An embedding is a function that maps entities from one algebraic structureinto another while preserving certain characteristics. Embeddings are beingused successfully for mapping relational data or text into vector spaces wherethey can be used for machine learning, similarity search, or similar tasks.Weaddress the problem of finding vector space embeddings for theories in theDescription Logic $\mathcal{EL}^{++}$ that are also models of the TBox. To findsuch embeddings, we define an optimization problem that characterizes themodel-theoretic semantics of the operators in $\mathcal{EL}^{++}$ within$\Re^n$, thereby solving the problem of finding an interpretation function foran $\mathcal{EL}^{++}$ theory given a particular domain $\Delta$. Our approachis mainly relevant to large $\mathcal{EL}^{++}$ theories and knowledge basessuch as the ontologies and knowledge graphs used in the life sciences. Wedemonstrate that our method can be used for improved prediction ofprotein--protein interactions when compared to semantic similarity measures orknowledge graph embedding",EL Embeddings: Geometric construction of models for the Description Logic EL ++,https://arxiv.org/abs/1902.10499v1,https://arxiv.org/pdf/1902.10499v1.pdf,422,https://github.com/bio-ontology-research-group/el-embeddings,3,Machine Translation,,"graph embedding,knowledge graph embedding,knowledge graphs,semantic textual similarity"
,,"    Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. Unlike most existing approaches that only focus on leveraging knowledge graphs for more accurate recommendation, we perform explicit reasoning with knowledge for decision making so that the recommendations are generated and supported by an interpretable causal inference procedure.To this end, we propose a method called Policy-Guided Path Reasoning (PGPR), which couples recommendation and interpretability by providing actual paths in a knowledge graph. Our contributions include four aspects. We first highlight the significance of incorporating knowledge graphs into recommendation to formally define and interpret the reasoning process. Second, we propose a reinforcement learning (RL) approach featuring an innovative soft reward strategy, user-conditional action pruning and a multi-hop scoring function. Third, we design a policy-guided graph search algorithm to efficiently and effectively sample reasoning paths for recommendation. Finally, we extensively evaluate our method on several large-scale real-world benchmark datasets, obtaining favorable results compared with state-of-the-art methods.",Reinforcement Knowledge Graph Reasoning for Explainable Recommendation,https://arxiv.org/abs/1906.05237v1,https://arxiv.org/pdf/1906.05237v1.pdf,422,https://github.com/orcax/PGPR,9,Machine Translation,,"causal inference,decision making,knowledge graphs"
,,"    We present RUDIK, a system for the discovery of declarative rules over knowledge-bases (KBs). RUDIK discovers rules that express positive relationships between entities, such as if two persons have the same parent, they are siblings, and negative rules, i.e., patterns that identify contradictions in the data, such as if two persons are married, one cannot be the child of the other.While the former class infers new facts in the KB, the latter class is crucial for other tasks, such as detecting erroneous triples in data cleaning, or the creation of negative examples to bootstrap learning algorithms. The system is designed to: (i) enlarge the expressive power of the rule language to obtain complex rules and wide coverage of the facts in the KB, (ii) discover approximate rules (soft constraints) to be robust to errors and incompleteness in the KB, (iii) use disk-based algorithms, effectively enabling rule mining in commodity machines. In contrast with traditional ranking of all rules based on a measure of support, we propose an approach to identify the subset of useful rules to be exposed to the user. We model the mining process as an incremental graph exploration problem and prove that our search strategy has guarantees on the optimality of the results. We have conducted extensive experiments using real-world KBs to show that RUDIK outperforms previous proposals in terms of efficiency and that it discovers more effective rules for the application at hand.",Robust Discovery of Positive and Negative Rules in Knowledge-Bases,http://www.eurecom.fr/fr/publication/5469/detail/robust-discovery-of-positive-and-negative-rules-in-knowledge-bases-1,https://www.dropbox.com/s/4hgcli75ccqe20t/Rudik_CR_ICDE.pdf?dl=0,422,https://github.com/stefano-ortona/rudik,4,Machine Translation,,"knowledge graphs,knowledge graphs data curation"
,,"    SPARQL is a highly powerful query language for an ever-growing number of Linked Data resources and Knowledge Graphs. Using it requires a certain familiarity with the entities in the domain to be queried as well as expertise in the language's syntax and semantics, none of which average human web users can be assumed to possess.To overcome this limitation, automatically translating natural language questions to SPARQL queries has been a vibrant field of research. However, to this date, the vast success of deep learning methods has not yet been fully propagated to this research problem. This paper contributes to filling this gap by evaluating the utilization of eight different Neural Machine Translation (NMT) models for the task of translating from natural language to the structured query language SPARQL. While highlighting the importance of high-quantity and high-quality datasets, the results show a dominance of a CNN-based architecture with a BLEU score of up to 98 and accuracy of up to 94%.",Neural Machine Translating from Natural Language to SPARQL,https://arxiv.org/abs/1906.09302v1,https://arxiv.org/pdf/1906.09302v1.pdf,422,https://github.com/AKSW/DBNQA,4,Machine Translation,,"knowledge graphs,machine translation"
,,"    SPARQL is a highly powerful query language for an ever-growing number of Linked Data resources and Knowledge Graphs. Using it requires a certain familiarity with the entities in the domain to be queried as well as expertise in the language's syntax and semantics, none of which average human web users can be assumed to possess.To overcome this limitation, automatically translating natural language questions to SPARQL queries has been a vibrant field of research. However, to this date, the vast success of deep learning methods has not yet been fully propagated to this research problem. This paper contributes to filling this gap by evaluating the utilization of eight different Neural Machine Translation (NMT) models for the task of translating from natural language to the structured query language SPARQL. While highlighting the importance of high-quantity and high-quality datasets, the results show a dominance of a CNN-based architecture with a BLEU score of up to 98 and accuracy of up to 94%.",Neural Machine Translating from Natural Language to SPARQL,https://arxiv.org/abs/1906.09302v1,https://arxiv.org/pdf/1906.09302v1.pdf,422,https://github.com/AKSW/DBNQA,4,Machine Translation,,"knowledge graphs,machine translation"
,,"    Knowledge graph embedding aims to embed entities and relations of knowledgegraphs into low-dimensional vector spaces. Translating embedding methods regardrelations as the translation from head entities to tail entities, which achievethe state-of-the-art results among knowledge graph embedding methods.However,a major limitation of these methods is the time consuming training process,which may take several days or even weeks for large knowledge graphs, andresult in great difficulty in practical applications. In this paper, we proposean efficient parallel framework for translating embedding methods, calledParTrans-X, which enables the methods to be paralleled without locks byutilizing the distinguished structures of knowledge graphs. Experiments on twodatasets with three typical translating embedding methods, i.e., TransE [3],TransH [17], and a more efficient variant TransE- AdaGrad [10] validate thatParTrans-X can speed up the training process by more than an order ofmagnitude.",Efficient Parallel Translating Embedding For Knowledge Graphs,https://arxiv.org/abs/1703.10316v4,https://arxiv.org/pdf/1703.10316v4.pdf,422,https://github.com/zdh2292390/ParTrans-X,5,Machine Translation,,"graph embedding,knowledge graph embedding,knowledge graphs"
,,"    Latent factor models are increasingly popular for modeling multi-relationalknowledge graphs. By their vectorial nature, it is not only hard to interpretwhy this class of models works so well, but also to understand where they failand how they might be improved.We conduct an experimental survey ofstate-of-the-art models, not towards a purely comparative end, but as a meansto get insight about their inductive abilities. To assess the strengths andweaknesses of each model, we create simple tasks that exhibit first, atomicproperties of binary relations, and then, common inter-relational inferencethrough synthetic genealogies. Based on these experimental results, we proposenew research directions to improve on existing models.",On Inductive Abilities of Latent Factor Models for Relational Learning,https://arxiv.org/abs/1709.05666v1,https://arxiv.org/pdf/1709.05666v1.pdf,422,https://github.com/ttrouill/induction_experiments,6,Machine Translation,,"knowledge graphs,relational reasoning"
,,"    Knowledge graph embedding has been an active research topic for knowledgebase completion, with progressive improvement from the initial TransE, TransH,DistMult et al to the current state-of-the-art ConvE. ConvE uses 2D convolutionover embeddings and multiple layers of nonlinear features to model knowledgegraphs.The model can be efficiently trained and scalable to large knowledgegraphs. However, there is no structure enforcement in the embedding space ofConvE. The recent graph convolutional network (GCN) provides another way oflearning graph node embedding by successfully utilizing graph connectivitystructure. In this work, we propose a novel end-to-end Structure-AwareConvolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN),and a decoder of a convolutional network called Conv-TransE. WGCN utilizesknowledge graph node structure, node attributes and edge relation types. It haslearnable weights that adapt the amount of information from neighbors used inlocal aggregation, leading to more accurate embeddings of graph nodes. Nodeattributes in the graph are represented as additional nodes in the WGCN. Thedecoder Conv-TransE enables the state-of-the-art ConvE to be translationalbetween entities and relations while keeps the same link prediction performanceas ConvE. We demonstrate the effectiveness of the proposed SACN on standardFB15k-237 and WN18RR datasets, and it gives about 10% relative improvement overthe state-of-the-art ConvE in terms of ",End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion,https://arxiv.org/abs/1811.04441v2,https://arxiv.org/pdf/1811.04441v2.pdf,422,https://github.com/JD-AI-Research-Silicon-Valley/SACN,18,Machine Translation,,"graph embedding,knowledge base completion,knowledge graph embedding,knowledge graphs,link prediction"
,,"    Representing entities and relations in an embedding space is a well-studiedapproach for machine learning on relational data. Existing approaches, however,primarily focus on simple link structure between a finite set of entities,ignoring the variety of data types that are often used in knowledge bases, suchas text, images, and numerical values.In this paper, we propose multimodalknowledge base embeddings (MKBE) that use different neural encoders for thisvariety of observed data, and combine them with existing relational models tolearn embeddings of the entities and multimodal data. Further, using theselearned embedings and different neural decoders, we introduce a novelmultimodal imputation model to generate missing multimodal values, like textand images, from information in the knowledge base. We enrich existingrelational datasets to create two novel benchmarks that contain additionalinformation such as textual descriptions and images of the original entities. We demonstrate that our models utilize this additional information effectivelyto provide more accurate link prediction, achieving state-of-the-art resultswith a considerable gap of 5-7% over existing methods. Further, we evaluate thequality of our generated multimodal values via a user study. We have releasethe datasets and the open-source implementation of our models athttps://github.com/pouyapez/mkbe",Embedding Multimodal Relational Data for Knowledge Base Completion,https://arxiv.org/abs/1809.01341v2,https://arxiv.org/pdf/1809.01341v2.pdf,422,"https://github.com/pouyapez/multim-kb-embeddings,https://github.com/pouyapez/mkbe","31,31",Machine Translation,,"imputation,knowledge base completion,link prediction"
,,"    Knowledge bases of real-world facts about entities and their relationshipsare useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to beable to perform link prediction or knowledge base completion, i.e., predictwhether a relationship not in the knowledge base is likely to be true.Thispaper combines insights from several previous link prediction models into a newembedding model STransE that represents each entity as a low-dimensionalvector, and each relation by two matrices and a translation vector. STransE isa simple combination of the SE and TransE models, but it obtains better linkprediction performance on two benchmark datasets than previous embeddingmodels. Thus, STransE can serve as a new baseline for the more complex modelsin the link prediction task.",STransE: a novel embedding model of entities and relationships in knowledge bases,https://arxiv.org/abs/1606.08140v3,https://arxiv.org/pdf/1606.08140v3.pdf,422,https://github.com/datquocnguyen/STransE,34,Machine Translation,,"knowledge base completion,link prediction"
,,"    In this paper, we propose a novel embedding model, named ConvKB, forknowledge base completion. Our model ConvKB advances state-of-the-art models byemploying a convolutional neural network, so that it can capture globalrelationships and transitional characteristics between entities and relationsin knowledge bases.In ConvKB, each triple (head entity, relation, tail entity)is represented as a 3-column matrix where each column vector represents atriple element. This 3-column matrix is then fed to a convolution layer wheremultiple filters are operated on the matrix to generate different feature maps. These feature maps are then concatenated into a single feature vectorrepresenting the input triple. The feature vector is multiplied with a weightvector via a dot product to return a score. This score is then used to predictwhether the triple is valid or not. Experiments show that ConvKB achievesbetter link prediction performance than previous state-of-the-art embeddingmodels on two benchmark datasets WN18RR and FB15k-237.",A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network,https://arxiv.org/abs/1712.02121v2,https://arxiv.org/pdf/1712.02121v2.pdf,422,https://github.com/daiquocnguyen/ConvKB,68,Machine Translation,,"knowledge base completion,link prediction"
,,"    In this paper, we propose a novel embedding model, named ConvKB, forknowledge base completion. Our model ConvKB advances state-of-the-art models byemploying a convolutional neural network, so that it can capture globalrelationships and transitional characteristics between entities and relationsin knowledge bases.In ConvKB, each triple (head entity, relation, tail entity)is represented as a 3-column matrix where each column vector represents atriple element. This 3-column matrix is then fed to a convolution layer wheremultiple filters are operated on the matrix to generate different feature maps. These feature maps are then concatenated into a single feature vectorrepresenting the input triple. The feature vector is multiplied with a weightvector via a dot product to return a score. This score is then used to predictwhether the triple is valid or not. Experiments show that ConvKB achievesbetter link prediction performance than previous state-of-the-art embeddingmodels on two benchmark datasets WN18RR and FB15k-237.",A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network,https://arxiv.org/abs/1712.02121v2,https://arxiv.org/pdf/1712.02121v2.pdf,422,https://github.com/daiquocnguyen/ConvKB,68,Machine Translation,,"knowledge base completion,link prediction"
,,"    We present KBLRN, a framework for end-to-end learning of knowledge baserepresentations from latent, relational, and numerical features. KBLRNintegrates feature types with a novel combination of neural representationlearning and probabilistic product of experts models.To the best of ourknowledge, KBLRN is the first approach that learns representations of knowledgebases by integrating latent, relational, and numerical features. We show thatinstances of KBLRN outperform existing methods on a range of knowledge basecompletion tasks. We contribute a novel data sets enriching commonly usedknowledge base completion benchmarks with numerical features. The data sets areavailable under a permissive BSD-3 license. We also investigate the impactnumerical features have on the KB completion performance of KBLRN.","KBLRN : End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features",https://arxiv.org/abs/1709.04676v3,https://arxiv.org/pdf/1709.04676v3.pdf,422,https://github.com/nle-ml/mmkb,80,Machine Translation,,"knowledge base completion,representation learning"
,,"    We introduce KBGAN, an adversarial learning framework to improve theperformances of a wide range of existing knowledge graph embedding models. Because knowledge graphs typically only contain positive facts, sampling usefulnegative training examples is a non-trivial task.Replacing the head or tailentity of a fact with a uniformly randomly selected entity is a conventionalmethod for generating negative facts, but the majority of the generatednegative facts can be easily discriminated from positive facts, and willcontribute little towards the training. Inspired by generative adversarialnetworks (GANs), we use one knowledge graph embedding model as a negativesample generator to assist the training of our desired model, which acts as thediscriminator in GANs. This framework is independent of the concrete form ofgenerator and discriminator, and therefore can utilize a wide variety ofknowledge graph embedding models as its building blocks. In experiments, weadversarially train two translation-based models, TransE and TransD, each withassistance from one of the two probability-based models, DistMult and ComplEx. We evaluate the performances of KBGAN on the link prediction task, using threeknowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimentalresults show that adversarial training substantially improves the performancesof target embedding models under various settings.",KBGAN: Adversarial Learning for Knowledge Graph Embeddings,https://arxiv.org/abs/1711.04071v3,https://arxiv.org/pdf/1711.04071v3.pdf,422,"https://github.com/cai-lw/KBGAN,https://github.com/WeidongLi-KG/KBGAN_PyTorch-v0.4.1,https://github.com/Chenxr1997/KBGAN","151,8,0",Machine Translation,,"graph embedding,knowledge base completion,knowledge graph embedding,knowledge graph embeddings,knowledge graphs,link prediction"
,,"    This paper tackles the problem of endogenous link prediction for KnowledgeBase completion. Knowledge Bases can be represented as directed graphs whosenodes correspond to entities and edges to relationships.Previous attemptseither consist of powerful systems with high capacity to model complexconnectivity patterns, which unfortunately usually end up overfitting on rarerelationships, or in approaches that trade capacity for simplicity in order tofairly model all relationships, frequent or not. In this paper, we proposeTatec a happy medium obtained by complementing a high-capacity model with asimpler one, both pre-trained separately and then combined. We present severalvariants of this model with different kinds of regularization and combinationstrategies and show that this approach outperforms existing methods ondifferent types of relationships by achieving state-of-the-art results on fourbenchmarks of the literature.",Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases,https://arxiv.org/abs/1506.00999v1,https://arxiv.org/pdf/1506.00999v1.pdf,422,"https://github.com/glorotxa/SME,https://github.com/usherwang02/SemanticMatchingEnergy-Theano","211,1",Machine Translation,,"knowledge base completion,link prediction"
,,"    Knowledge graphs enable a wide variety of applications, including questionanswering and information retrieval. Despite the great effort invested in theircreation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata)remain incomplete.We introduce Relational Graph Convolutional Networks(R-GCNs) and apply them to two standard knowledge base completion tasks: Linkprediction (recovery of missing facts, i.e. subject-predicate-object triples)and entity classification (recovery of missing entity attributes). R-GCNs arerelated to a recent class of neural networks operating on graphs, and aredeveloped specifically to deal with the highly multi-relational datacharacteristic of realistic knowledge bases. We demonstrate the effectivenessof R-GCNs as a stand-alone model for entity classification. We further showthat factorization models for link prediction such as DistMult can besignificantly improved by enriching them with an encoder model to accumulateevidence over multiple inference steps in the relational graph, demonstrating alarge improvement of 29.8% on FB15k-237 over a decoder-only baseline.",Modeling Relational Data with Graph Convolutional Networks,https://arxiv.org/abs/1703.06103v4,https://arxiv.org/pdf/1703.06103v4.pdf,422,"https://github.com/tkipf/gae,https://github.com/Microsoft/gated-graph-neural-network-samples,https://github.com/tkipf/relational-gcn,https://github.com/MichSchli/RelationPrediction,https://github.com/INK-USC/RE-Net","722,636,368,118,48",Machine Translation,,"information retrieval,knowledge base completion,knowledge graphs,link prediction"
,,"    Content-based news recommendation systems need to recommend news articles based on the topics and content of articles without using user specific information. Many news articles describe the occurrence of specific events and named entities including people, places or objects.In this paper, we propose a graph traversal algorithm as well as a novel weighting scheme for cold-start content based news recommendation utilizing these named entities. Seeking to create a higher degree of user-specific relevance, our algorithm computes the shortest distance between named entities, across news articles, over a large knowledge graph. Moreover, we have created a new human annotated data set for evaluating content based news recommendation systems. Experimental results show our method is suitable to tackle the hard cold-start problem and it produces stronger Pearson correlation to human similarity scores than other cold-start methods. Our method is also complementary and a combination with the conventional cold-start recommendation methods may yield significant performance gains. The dataset, CNRec, is available at: https://github.com/kevinj22/CNRec",Content based News Recommendation via Shortest Entity Distance over Knowledge Graphs,https://arxiv.org/abs/1905.13132v1,https://arxiv.org/pdf/1905.13132v1.pdf,422,https://github.com/kevinj22/CNRec,0,Machine Translation,,"knowledge graphs,recommendation systems"
,,"    Entity summarization aims at creating brief but informative descriptions of entities from knowledge graphs. While previous work mostly focused on traditional techniques such as clustering algorithms and graph models, we ask how to apply deep learning methods into this task.In this paper we propose ESA, a neural network with supervised attention mechanisms for entity summarization. Specifically, we calculate attention weights for facts in each entity, and rank facts to generate reliable summaries. We explore techniques to solve difficult learning problems presented by the ESA, and demonstrate the effectiveness of our model in comparison with the state-of-the-art methods. Experimental results show that our model improves the quality of the entity summaries in both F-measure and MAP.",ESA: Entity Summarization with Attention,https://arxiv.org/abs/1905.10625v1,https://arxiv.org/pdf/1905.10625v1.pdf,422,https://github.com/WeiDongjunGabriel/ESA,0,Machine Translation,,knowledge graphs
,,"    Entity summarization aims at creating brief but informative descriptions of entities from knowledge graphs. While previous work mostly focused on traditional techniques such as clustering algorithms and graph models, we ask how to apply deep learning methods into this task.In this paper we propose ESA, a neural network with supervised attention mechanisms for entity summarization. Specifically, we calculate attention weights for facts in each entity, and rank facts to generate reliable summaries. We explore techniques to solve difficult learning problems presented by the ESA, and demonstrate the effectiveness of our model in comparison with the state-of-the-art methods. Experimental results show that our model improves the quality of the entity summaries in both F-measure and MAP.",ESA: Entity Summarization with Attention,https://arxiv.org/abs/1905.10625v1,https://arxiv.org/pdf/1905.10625v1.pdf,422,https://github.com/WeiDongjunGabriel/ESA,0,Machine Translation,,knowledge graphs
,,"    The problem of Knowledge Base Completion can be framed as a 3rd-order binarytensor completion problem. In this light, the Canonical Tensor Decomposition(CP) (Hitchcock, 1927) seems like a natural solution; however, currentimplementations of CP on standard Knowledge Base Completion benchmarks arelagging behind their competitors.In this work, we attempt to understand thelimits of CP for knowledge base completion. First, we motivate and test a novelregularizer, based on tensor nuclear $p$-norms. Then, we present areformulation of the problem that makes it invariant to arbitrary choices inthe inclusion of predicates or their reciprocals in the dataset. These twomethods combined allow us to beat the current state of the art on severaldatasets with a CP decomposition, and obtain even better results using the moreadvanced ComplEx model.",Canonical Tensor Decomposition for Knowledge Base Completion,https://arxiv.org/abs/1806.07297v1,https://arxiv.org/pdf/1806.07297v1.pdf,422,https://github.com/facebookresearch/kbc,62,Machine Translation,,"knowledge base completion,link prediction"
,,"    Despite being vast repositories of factual information, cross-domainknowledge graphs, such as Wikidata and the Google Knowledge Graph, onlysparsely provide short synoptic descriptions for entities. Such descriptionsthat briefly identify the most discernible features of an entity providereaders with a near-instantaneous understanding of what kind of entity they arebeing presented.They can also aid in tasks such as named entitydisambiguation, ontological type determination, and answering entity queries. Given the rapidly increasing numbers of entities in knowledge graphs, a fullyautomated synthesis of succinct textual descriptions from underlying factualinformation is essential. To this end, we propose a novel fact-to-sequenceencoder-decoder model with a suitable copy mechanism to generate concise andprecise textual descriptions of entities. In an in-depth evaluation, wedemonstrate that our method significantly outperforms state-of-the-artalternatives.",Be Concise and Precise: Synthesizing Open-Domain Entity Descriptions from Facts,https://arxiv.org/abs/1904.07391v1,https://arxiv.org/pdf/1904.07391v1.pdf,422,https://github.com/kingsaint/Wikidata-Descriptions,1,Machine Translation,,knowledge graphs
,,"    Knowledge graph (KG) embedding techniques use structured relationships between entities to learn low-dimensional representations of entities and relations. One prominent goal of these approaches is to improve the quality of knowledge graphs by removing errors and adding missing facts.Surprisingly, most embedding techniques have been evaluated on benchmark datasets consisting of dense and reliable subsets of human-curated KGs, which tend to be fairly complete and have few errors. In this paper, we consider the problem of applying embedding techniques to KGs extracted from text, which are often incomplete and contain errors. We compare the sparsity and unreliability of different KGs and perform empirical experiments demonstrating how embedding approaches degrade as sparsity and unreliability increase.",Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short,https://www.aclweb.org/anthology/papers/D/D17/D17-1184/,https://www.aclweb.org/anthology/D17-1184,422,https://github.com/linqs/pujara-emnlp17,1,Machine Translation,,"knowledge graph embeddings,knowledge graphs,question answering"
,,"    Knowledge graph (KG) is known to be helpful for the task of questionanswering (QA), since it provides well-structured relational informationbetween entities, and allows one to further infer indirect facts. However, itis challenging to build QA systems which can learn to reason over knowledgegraphs based on question-answer pairs alone.First, when people ask questions,their expressions are noisy (for example, typos in texts, or variations inpronunciations), which is non-trivial for the QA system to match thosementioned entities to the knowledge graph. Second, many questions requiremulti-hop logic reasoning over the knowledge graph to retrieve the answers. Toaddress these challenges, we propose a novel and unified deep learningarchitecture, and an end-to-end variational learning algorithm which can handlenoise in questions, and learn multi-hop reasoning simultaneously. Our methodachieves state-of-the-art performance on a recent benchmark dataset in theliterature. We also derive a series of new benchmark datasets, includingquestions for multi-hop reasoning, questions paraphrased by neural translationmodel, and questions in human voice. Our method yields very promising resultson all these challenging datasets.",Variational Reasoning for Question Answering with Knowledge Graph,https://arxiv.org/abs/1709.04071v5,https://arxiv.org/pdf/1709.04071v5.pdf,422,https://github.com/yuyuz/Variational-Reasoning-Networks,2,Machine Translation,,"knowledge graphs,question answering"
,,"    Knowledge graph (KG) is known to be helpful for the task of questionanswering (QA), since it provides well-structured relational informationbetween entities, and allows one to further infer indirect facts. However, itis challenging to build QA systems which can learn to reason over knowledgegraphs based on question-answer pairs alone.First, when people ask questions,their expressions are noisy (for example, typos in texts, or variations inpronunciations), which is non-trivial for the QA system to match thosementioned entities to the knowledge graph. Second, many questions requiremulti-hop logic reasoning over the knowledge graph to retrieve the answers. Toaddress these challenges, we propose a novel and unified deep learningarchitecture, and an end-to-end variational learning algorithm which can handlenoise in questions, and learn multi-hop reasoning simultaneously. Our methodachieves state-of-the-art performance on a recent benchmark dataset in theliterature. We also derive a series of new benchmark datasets, includingquestions for multi-hop reasoning, questions paraphrased by neural translationmodel, and questions in human voice. Our method yields very promising resultson all these challenging datasets.",Variational Reasoning for Question Answering with Knowledge Graph,https://arxiv.org/abs/1709.04071v5,https://arxiv.org/pdf/1709.04071v5.pdf,422,https://github.com/yuyuz/Variational-Reasoning-Networks,2,Machine Translation,,"knowledge graphs,question answering"
